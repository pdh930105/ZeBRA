{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, time, random\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from utils import AverageMeter, RecorderMeter, time_string, convert_secs2time, clustering_loss, change_quan_bitwidth\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## import BFA module\n",
    "import models\n",
    "from models.quantization import quan_Conv2d, quan_Linear, quantize\n",
    "from attack.BFA import *\n",
    "\n",
    "## import gradcam module\n",
    "\n",
    "from gradcam.gradcam import GradCAM\n",
    "from gradcam.gradcam_utils import visualize_cam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet test\n",
    "manualSeed=5\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "net = models.mobilenet_v2_quan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, summary_output=False):\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    output_summary = [] # init a list for output summary\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(tqdm(val_loader)):\n",
    "            target = target.cuda()\n",
    "            input = input.cuda()\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # summary the output\n",
    "            if summary_output:\n",
    "                tmp_list = output.max(1, keepdim=True)[1].flatten().cpu().numpy() # get the index of the max log-probability\n",
    "                output_summary.append(tmp_list)\n",
    "\n",
    "\n",
    "            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "            top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        print(\n",
    "            '  **Test** Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Error@1 {error1:.3f}'\n",
    "            .format(top1=top1, top5=top5, error1=100 - top1.avg))\n",
    "        \n",
    "    if summary_output:\n",
    "        output_summary = np.asarray(output_summary).flatten()\n",
    "        return top1.avg, top5.avg, losses.avg, output_summary\n",
    "    else:\n",
    "        return top1.avg, top5.avg, losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_attack(attacker, model, model_clean, attack_data, attack_label, test_loader,\n",
    "                   N_iter, csv_save_path=None, random_attack=False):\n",
    "    # Note that, attack has to be done in evaluation model due to batch-norm.\n",
    "    # see: https://discuss.pytorch.org/t/what-does-model-eval-do-for-batchnorm-layer/7146\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    iter_time = AverageMeter()\n",
    "    attack_time = AverageMeter()\n",
    "\n",
    "\n",
    "    # attempt to use the training data to conduct BFA\n",
    "    attack_label=attack_label.cuda()\n",
    "    attack_data = attack_data.cuda()\n",
    "    # Override the target to prevent label leaking\n",
    "    \n",
    "    # evaluate the test accuracy of clean model\n",
    "    val_acc_top1, val_acc_top5, val_loss = validate(test_loader, model,\n",
    "                                                    attacker.criterion)\n",
    "\n",
    "    end = time.time()\n",
    "    \n",
    "    df = pd.DataFrame() #init a empty dataframe for logging\n",
    "    last_val_acc_top1 = val_acc_top1\n",
    "        # Stop the attack if the accuracy is below the configured break_acc.\n",
    "    break_acc = 0.2\n",
    "\n",
    "\n",
    "    for i_iter in range(N_iter):\n",
    "        print('**********************************')\n",
    "        if not random_attack:\n",
    "            attack_log = attacker.progressive_bit_search(model, attack_data, attack_label)\n",
    "        else:\n",
    "            attack_log = attacker.random_flip_one_bit(model)\n",
    "            \n",
    "        \n",
    "        # measure data loading time\n",
    "        attack_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        h_dist = hamming_distance(model, model_clean)\n",
    "\n",
    "        # record the loss\n",
    "        if hasattr(attacker, \"loss_max\"):\n",
    "            losses.update(attacker.loss_max, attack_data.size(0))\n",
    "\n",
    "        print(\n",
    "            'Iteration: [{:03d}/{:03d}]   '\n",
    "            'Attack Time {attack_time.val:.3f} ({attack_time.avg:.3f})  '.\n",
    "            format((i_iter + 1),\n",
    "                   N_iter,\n",
    "                   attack_time=attack_time,\n",
    "                   iter_time=iter_time) + time_string())\n",
    "        try:\n",
    "            print('loss before attack: {:.4f}'.format(attacker.loss.item()))\n",
    "            print('loss after attack: {:.4f}'.format(attacker.loss_max))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #print_log('bit flips: {:.0f}'.format(attacker.bit_counter), log)\n",
    "        print('hamming_dist: {:.0f}'.format(h_dist))\n",
    "\n",
    "        # exam the BFA on entire val dataset\n",
    "        val_acc_top1, val_acc_top5, val_loss = validate(\n",
    "            test_loader, model, attacker.criterion)\n",
    "        \n",
    "        \n",
    "        # add additional info for logging\n",
    "        acc_drop = last_val_acc_top1 - val_acc_top1\n",
    "        last_val_acc_top1 = val_acc_top1\n",
    "        \n",
    "        for i in range(attack_log.__len__()):\n",
    "            attack_log[i].append(val_acc_top1)\n",
    "            attack_log[i].append(acc_drop)\n",
    "        \n",
    "        df = df.append(attack_log, ignore_index=True)\n",
    "\n",
    "        # measure elapsed time\n",
    "        iter_time.update(time.time() - end)\n",
    "        print(\n",
    "            'iteration Time {iter_time.val:.3f} ({iter_time.avg:.3f})'.format(\n",
    "                iter_time=iter_time))\n",
    "        end = time.time()\n",
    "\n",
    "        if val_acc_top1 <= break_acc:\n",
    "            break\n",
    "        \n",
    "    # attack profile\n",
    "    column_list = ['module idx', 'bit-flip idx', 'module name', 'weight idx',\n",
    "                  'weight before attack', 'weight after attack', 'validation accuracy',\n",
    "                  'accuracy drop']\n",
    "    df.columns = column_list\n",
    "    df['trial seed'] = manualSeed\n",
    "    if csv_save_path is not None:\n",
    "        csv_file_name = 'bfa_attack_profile_{}.csv'.format(manualSeed)\n",
    "        export_csv = df.to_csv(os.path.join(csv_save_path, csv_file_name), index=None)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader setting\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])  # here is actually the validation dataset\n",
    "\n",
    "test_dir = \"/dataset/ImageNet/Classification/val\"\n",
    "test_data = dset.ImageFolder(test_dir, transform=test_transform)\n",
    "num_classes = 1000\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=128,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=8,\n",
    "                                              pin_memory=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# separate the parameters thus param groups can be updated by different optimizer\n",
    "all_param = [\n",
    "    param for name, param in net.named_parameters()\n",
    "    if not 'step_size' in name\n",
    "]\n",
    "# FP로 되어있는 weight를 quantization 해서 int * step_size 로 변경\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공격할 데이터 미리 뽑기\n",
    "attack_data, attack_label = next(iter(test_loader))\n",
    "attack_data, attack_label = attack_data.cuda(), attack_label.cuda()\n",
    "\n",
    "net_clean = copy.deepcopy(net)\n",
    "for m in net_clean.modules():\n",
    "    if isinstance(m, quan_Conv2d) or isinstance(m, quan_Linear):\n",
    "        # simple step size update based on the pretrained model or weight init\n",
    "        m.__reset_stepsize__()\n",
    "        m.__reset_weight__()\n",
    "        \n",
    "net_attack = copy.deepcopy(net_clean)\n",
    "net_clean = net_clean.cuda()\n",
    "net_attack = net_attack.cuda()\n",
    "attacker = BFA(criterion, net_attack, k_top=10) # 최대 10개의 gradient가 가장 큰 weight를 보고 비교\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n100%|██████████| 391/391 [01:03<00:00,  6.15it/s]\\n  **Test** Prec@1 71.120 Prec@5 90.026 Error@1 28.880\\n**********************************\\n/root/torch/hardware_attack/ZeBRA/attack/BFA.py:51: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\\n  b_bin_topk = (w_bin_topk.repeat(m.N_bits,1) & m.b_w.abs().repeat(1,k_top).short()) /root/torch/hardware_attack/ZeBRA/attack/data_conversion.py:54: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\\n  counter += ((t & 2**i) // 2**i).sum()\\nattacked module: features.1.conv.0.0\\nattacked weight index: [4 0 2 1]\\nweight before attack: -102.0\\nweight after attack: 26.0\\nIteration: [001/010]   Attack Time 1.677 (1.677)  [2022-04-03 18:42:51]\\nloss before attack: 1.5423\\nloss after attack: 16.0514\\nhamming_dist: 1\\n100%|██████████| 391/391 [01:04<00:00,  6.05it/s]  **Test** Prec@1 0.122 Prec@5 0.732 Error@1 99.878\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perform_attack(attacker, net_attack, net_clean, attack_data, attack_label, test_loader, 10, csv_save_path=None, random_attack=False)\n",
    "\n",
    "\"\"\"\n",
    "100%|██████████| 391/391 [01:03<00:00,  6.15it/s]\n",
    "  **Test** Prec@1 71.120 Prec@5 90.026 Error@1 28.880\n",
    "**********************************\n",
    "/root/torch/hardware_attack/ZeBRA/attack/BFA.py:51: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
    "  b_bin_topk = (w_bin_topk.repeat(m.N_bits,1) & m.b_w.abs().repeat(1,k_top).short()) \\\n",
    "/root/torch/hardware_attack/ZeBRA/attack/data_conversion.py:54: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
    "  counter += ((t & 2**i) // 2**i).sum()\n",
    "attacked module: features.1.conv.0.0\n",
    "attacked weight index: [4 0 2 1]\n",
    "weight before attack: -102.0\n",
    "weight after attack: 26.0\n",
    "Iteration: [001/010]   Attack Time 1.677 (1.677)  [2022-04-03 18:42:51]\n",
    "loss before attack: 1.5423\n",
    "loss after attack: 16.0514\n",
    "hamming_dist: 1\n",
    "100%|██████████| 391/391 [01:04<00:00,  6.05it/s]  **Test** Prec@1 0.122 Prec@5 0.732 Error@1 99.878\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count : 0, name : features.0.0, weight shape : torch.Size([32, 3, 3, 3])\n",
      "count : 1, name : features.1.conv.0.0, weight shape : torch.Size([32, 1, 3, 3])\n",
      "count : 2, name : features.1.conv.1, weight shape : torch.Size([16, 32, 1, 1])\n",
      "count : 3, name : features.2.conv.0.0, weight shape : torch.Size([96, 16, 1, 1])\n",
      "count : 4, name : features.2.conv.1.0, weight shape : torch.Size([96, 1, 3, 3])\n",
      "count : 5, name : features.2.conv.2, weight shape : torch.Size([24, 96, 1, 1])\n",
      "count : 6, name : features.3.conv.0.0, weight shape : torch.Size([144, 24, 1, 1])\n",
      "count : 7, name : features.3.conv.1.0, weight shape : torch.Size([144, 1, 3, 3])\n",
      "count : 8, name : features.3.conv.2, weight shape : torch.Size([24, 144, 1, 1])\n",
      "count : 9, name : features.4.conv.0.0, weight shape : torch.Size([144, 24, 1, 1])\n",
      "count : 10, name : features.4.conv.1.0, weight shape : torch.Size([144, 1, 3, 3])\n",
      "count : 11, name : features.4.conv.2, weight shape : torch.Size([32, 144, 1, 1])\n",
      "count : 12, name : features.5.conv.0.0, weight shape : torch.Size([192, 32, 1, 1])\n",
      "count : 13, name : features.5.conv.1.0, weight shape : torch.Size([192, 1, 3, 3])\n",
      "count : 14, name : features.5.conv.2, weight shape : torch.Size([32, 192, 1, 1])\n",
      "count : 15, name : features.6.conv.0.0, weight shape : torch.Size([192, 32, 1, 1])\n",
      "count : 16, name : features.6.conv.1.0, weight shape : torch.Size([192, 1, 3, 3])\n",
      "count : 17, name : features.6.conv.2, weight shape : torch.Size([32, 192, 1, 1])\n",
      "count : 18, name : features.7.conv.0.0, weight shape : torch.Size([192, 32, 1, 1])\n",
      "count : 19, name : features.7.conv.1.0, weight shape : torch.Size([192, 1, 3, 3])\n",
      "count : 20, name : features.7.conv.2, weight shape : torch.Size([64, 192, 1, 1])\n",
      "count : 21, name : features.8.conv.0.0, weight shape : torch.Size([384, 64, 1, 1])\n",
      "count : 22, name : features.8.conv.1.0, weight shape : torch.Size([384, 1, 3, 3])\n",
      "count : 23, name : features.8.conv.2, weight shape : torch.Size([64, 384, 1, 1])\n",
      "count : 24, name : features.9.conv.0.0, weight shape : torch.Size([384, 64, 1, 1])\n",
      "count : 25, name : features.9.conv.1.0, weight shape : torch.Size([384, 1, 3, 3])\n",
      "count : 26, name : features.9.conv.2, weight shape : torch.Size([64, 384, 1, 1])\n",
      "count : 27, name : features.10.conv.0.0, weight shape : torch.Size([384, 64, 1, 1])\n",
      "count : 28, name : features.10.conv.1.0, weight shape : torch.Size([384, 1, 3, 3])\n",
      "count : 29, name : features.10.conv.2, weight shape : torch.Size([64, 384, 1, 1])\n",
      "count : 30, name : features.11.conv.0.0, weight shape : torch.Size([384, 64, 1, 1])\n",
      "count : 31, name : features.11.conv.1.0, weight shape : torch.Size([384, 1, 3, 3])\n",
      "count : 32, name : features.11.conv.2, weight shape : torch.Size([96, 384, 1, 1])\n",
      "count : 33, name : features.12.conv.0.0, weight shape : torch.Size([576, 96, 1, 1])\n",
      "count : 34, name : features.12.conv.1.0, weight shape : torch.Size([576, 1, 3, 3])\n",
      "count : 35, name : features.12.conv.2, weight shape : torch.Size([96, 576, 1, 1])\n",
      "count : 36, name : features.13.conv.0.0, weight shape : torch.Size([576, 96, 1, 1])\n",
      "count : 37, name : features.13.conv.1.0, weight shape : torch.Size([576, 1, 3, 3])\n",
      "count : 38, name : features.13.conv.2, weight shape : torch.Size([96, 576, 1, 1])\n",
      "count : 39, name : features.14.conv.0.0, weight shape : torch.Size([576, 96, 1, 1])\n",
      "count : 40, name : features.14.conv.1.0, weight shape : torch.Size([576, 1, 3, 3])\n",
      "count : 41, name : features.14.conv.2, weight shape : torch.Size([160, 576, 1, 1])\n",
      "count : 42, name : features.15.conv.0.0, weight shape : torch.Size([960, 160, 1, 1])\n",
      "count : 43, name : features.15.conv.1.0, weight shape : torch.Size([960, 1, 3, 3])\n",
      "count : 44, name : features.15.conv.2, weight shape : torch.Size([160, 960, 1, 1])\n",
      "count : 45, name : features.16.conv.0.0, weight shape : torch.Size([960, 160, 1, 1])\n",
      "count : 46, name : features.16.conv.1.0, weight shape : torch.Size([960, 1, 3, 3])\n",
      "count : 47, name : features.16.conv.2, weight shape : torch.Size([160, 960, 1, 1])\n",
      "count : 48, name : features.17.conv.0.0, weight shape : torch.Size([960, 160, 1, 1])\n",
      "count : 49, name : features.17.conv.1.0, weight shape : torch.Size([960, 1, 3, 3])\n",
      "count : 50, name : features.17.conv.2, weight shape : torch.Size([320, 960, 1, 1])\n",
      "count : 51, name : features.18.0, weight shape : torch.Size([1280, 320, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# visualize count, module_name, shape\n",
    "count = 0\n",
    "for name, m in net_clean.named_modules():\n",
    "    if isinstance(m, quan_Conv2d):\n",
    "        print(f\"count : {count}, name : {name}, weight shape : {m.weight.shape}\")\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:3610: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layer_idx = [1]\n",
    "\n",
    "visualize_image_count=1\n",
    "sample_data = attack_data[:visualize_image_count]\n",
    "\n",
    "\n",
    "for idx in layer_idx:        \n",
    "    model_dict = {'type':'imagenet', 'layer_target':idx, 'find_func':'count', 'arch':net_clean}\n",
    "    gradcam = GradCAM(model_dict)\n",
    "    score_map, logit = gradcam(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
